# Final exam place

this repository is for knowledge for final exam of Computer engineering - Open Informatics at CTU FEE Prague

  Contents:

  - [1. PAL - Polynomial algorithms for standard graph problems. Combinatorial and number-theoretical algorithms, isomorphism, prime numbers. Search trees and their use. Text search based on finite automata.](#1-pal---polynomial-algorithms-for-standard-graph-problems-combinatorial-and-number-theoretical-algorithms-isomorphism-prime-numbers-search-trees-and-their-use-text-search-based-on-finite-automata)
    - [ ] [1.1 Notation of asymptotic complexity of algorithms. Basic notation of graph problems - degree, path, circuit, cycle. Graph representations by adjacency, distance, Laplacian and incidence matrices. Adjacency list representation.](#11-notation-of-asymptotic-complexity-of-algorithms-basic-notation-of-graph-problems---degree-path-circuit-cycle-graph-representations-by-adjacency-distance-laplacian-and-incidence-matrices-adjacency-list-representation)
    - [ ] [1.2 Algorithms for minimum spanning tree (Prim-Jarník, Kruskal, Borůvka), strongly connected components (Kosaraju-Sharir, Tarjan), Euler trail. Union-find problem. Graph isomorphism, tree isomorphism.](#12-algorithms-for-minimum-spanning-tree-prim-jarník-kruskal-borůvka-strongly-connected-components-kosaraju-sharir-tarjan-euler-trail-union-find-problem-graph-isomorphism-tree-isomorphism)
    - [ ] [1.3 Generation and enumeration of combinatorial objects - subsets, k-element subsets, permutations. Gray codes. Prime numbers, sieve of Eratosthenes. Pseudorandom numbers properties. Linear congruential generator.](#13-generation-and-enumeration-of-combinatorial-objects---subsets-k-element-subsets-permutations-gray-codes-prime-numbers-sieve-of-eratosthenes-pseudorandom-numbers-properties-linear-congruential-generator)
    - [ ] [1.4 Search trees - data structures, operations, and their complexities. Binary tree, AVL tree, red-black tree (RB-tree), B-tree and B+ tree, splay tree, k-d tree. Nearest neighbor searching in k-d trees. Skip list.](#14-search-trees---data-structures-operations-and-their-complexities-binary-tree-avl-tree-red-black-tree-rb-tree-b-tree-and-b-tree-splay-tree-k-d-tree-nearest-neighbor-searching-in-k-d-trees-skip-list)
    - [ ] [1.5 Finite automata, regular expressions, operations over regular languages. Bit representation of nondeterministic finite automata. Text search algorithms - exact pattern matching, approximate pattern matching (Hamming and Levenshtein distance), dictionary automata.](#15-finite-automata-regular-expressions-operations-over-regular-languages-bit-representation-of-nondeterministic-finite-automata-text-search-algorithms---exact-pattern-matching-approximate-pattern-matching-hamming-and-levenshtein-distance-dictionary-automata)
  - [2. TAL - Problem/language complexity classes with respect to the time complexity of their solution and memory complexity including undecidable problems/languages.](#2-tal---problemlanguage-complexity-classes-with-respect-to-the-time-complexity-of-their-solution-and-memory-complexity-including-undecidable-problemslanguages)
    - [ ] [2.1 Asymptotic growth of functions, time and space complexity of algorithms. Correctness of algorithms - variant and invariant.](#21-asymptotic-growth-of-functions-time-and-space-complexity-of-algorithms-correctness-of-algorithms---variant-and-invariant)
    - [ ] [2.2 Deterministic Turing machines, multitape Turing machines, and Nondeterministic Turing machines.](#22-deterministic-turing-machines-multitape-turing-machines-and-nondeterministic-turing-machines)
    - [ ] [2.3 Decision problems and languages. Complexity classes P, NP, co-NP. Reduction and polynomial reduction, class NPC. Cook theorem. Heuristics and approximate algorithms for solving NP complete problems.](#23-decision-problems-and-languages-complexity-classes-p-np-co-np-reduction-and-polynomial-reduction-class-npc-cook-theorem-heuristics-and-approximate-algorithms-for-solving-np-complete-problems)
    - [ ] [2.4 Classes based on space complexity: PSPACE and NPSPACE. Savitch Theorem.](#24-classes-based-on-space-complexity-pspace-and-npspace-savitch-theorem)
    - [ ] [2.5 Randomized algorithms. Randomized Turing machines. Classes based on randomization: RP, ZPP, co-RP.](#25-randomized-algorithms-randomized-turing-machines-classes-based-on-randomization-rp-zpp-co-rp)
    - [ ] [2.6 Decidability and undecidability. Recursive and recursively enumerable languages. Diagonal language. Universal language and Universal Turing machine.](#26-decidability-and-undecidability-recursive-and-recursively-enumerab/le-languages-diagonal-language-universal-language-and-universal-turing-machine)
  - [3. KO - Combinatorial optimization problems - formulation, complexity analysis, algorithms and example applications.](#3-ko---combinatorial-optimization-problems---formulation-complexity-analysis-algorithms-and-example-applications)
    - [ ] [3.1 Integer Linear Programming. Shortest paths problem and traveling salesman problem ILP formulations. Branch and Bound algorithm. Problem formulations using ILP. Special ILP problems solvable in polynomial time.](#31-integer-linear-programming-shortest-paths-problem-and-traveling-salesman-problem-ilp-formulations-branch-and-bound-algorithm-problem-formulations-using-ilp-special-ilp-problems-solvable-in-polynomial-time)
    - [ ] [3.2 Shortest paths problem. Dijkstra, Bellman-Ford, and Floyd–Warshall algorithms. Shortest paths in directed acyclic graphs. Problem formulations using shortest paths.](#32-shortest-paths-problem-dijkstra-bellman-ford-and-floydwarshall-algorithms-shortest-paths-in-directed-acyclic-graphs-problem-formulations-using-shortest-paths)
    - [ ] [3.3 Network flows. Maximum flow and minimum cut problems. Ford-Fulkerson algorithm.  Feasible flow with balances. Minimum cost flow and cycle-canceling algorithm. Problem formulations using network flows. Maximum cardinality matching.](#33-network-flows-maximum-flow-and-minimum-cut-problems-ford-fulkerson-algorithm--feasible-flow-with-balances-minimum-cost-flow-and-cycle-canceling-algorithm-problem-formulations-using-network-flows-maximum-cardinality-matching)
    - [ ] [3.4 Knapsack problem. Approximation algorithm, dynamic programming approach, approximation scheme.](#34-knapsack-problem-approximation-algorithm-dynamic-programming-approach-approximation-scheme)
    - [ ] [3.5 Traveling salesman problem. Double-tree algorithm and Christofides algorithm for the metric problem. Local search k-OPT.](#35-traveling-salesman-problem-double-tree-algorithm-and-christofides-algorithm-for-the-metric-problem-local-search-k-opt)
    - [ ] [3.6 Scheduling - problem description and notation. One resource - Bratley algorithm, Horn algorithm. Parallel identical resources - list scheduling, dynamic programming. Project scheduling with temporal constraints - relative order and time-indexed ILP formulations.](#36-scheduling---problem-description-and-notation-one-resource---bratley-algorithm-horn-algorithm-parallel-identical-resources---list-scheduling-dynamic-programming-project-scheduling-with-temporal-constraints---relative-order-and-time-indexed-ilp-formulations)
    - [ ] [3.7 Constraint Satisfaction Problem. AC3 algorithm.](#37-constraint-satisfaction-problem-ac3-algorithm)
  - [4. ISC - Design and implementation of in-chip integrated systems, application specific systems.](#4-isc---design-and-implementation-of-in-chip-integrated-systems-application-specific-systems)
    - [ ] [4.1 Main features and economical aspects of the Application specific integrated circuits systems: full custom design, gate array, standard cells, programmable array logic;](#41-main-features-and-economical-aspects-of-the-application-specific-integrated-circuits-systems-full-custom-design-gate-array-standard-cells-programmable-array-logic)
    - [ ] [4.2 Design principles of mix-signal integrated circuits, purpose of hierarchical design, digital and analogue block interface, CAD design tools for automatic circuit generation; functional and static time analysis, formal verification; Verilog-A, Verilog-AMS, VHDL-A.](#42-design-principles-of-mix-signal-integrated-circuits-purpose-of-hierarchical-design-digital-and-analogue-block-interface-cad-design-tools-for-automatic-circuit-generation-functional-and-static-time-analysis-formal-verification-verilog-a-verilog-ams-vhdl-a)
    - [ ] [4.3 Front end design - functional specification, RTL, logic synthesis, Gate-level netlist, behavioral stimulus extraction.](#43-front-end-design---functional-specification-rtl-logic-synthesis-gate-level-netlist-behavioral-stimulus-extraction)
    - [ ] [4.4 Back End design - specification of Design Kit, Floorplanning, place and route, layout, parasitic extraction, layout versus schema check (LVS).](#44-back-end-design---specification-of-design-kit-floorplanning-place-and-route-layout-parasitic-extraction-layout-versus-schema-check-lvs)
    - [ ] [4.5 Tape out and IC fabrication process, integrated systems verification, scaling and design mapping to different technologies.](#45-tape-out-and-ic-fabrication-process-integrated-systems-verification-scaling-and-design-mapping-to-different-technologies)
  - [5. PAP - Advanced architectures of processors, memory and peripheral circuits and multiprocessor computers.](#5-pap---advanced-architectures-of-processors-memory-and-peripheral-circuits-and-multiprocessor-computers)
    - [ ] [5.1 Superscalar techniques used in nodes of multiprocessor systems, data flow inside the processor, Tomasulo algorithm and its deficiencies, precise exceptions support, architectural state, register renaming, reservation station, reorder buffer, instruction fetch, decode, dispatch, issue, execute, finish, complete, reorder, branch prediction, store forwarding, hit under miss.](#51-superscalar-techniques-used-in-nodes-of-multiprocessor-systems-data-flow-inside-the-processor-tomasulo-algorithm-and-its-deficiencies-precise-exceptions-support-architectural-state-register-renaming-reservation-station-reorder-buffer-instruction-fetch-decode-dispatch-issue-execute-finish-complete-reorder-branch-prediction-store-forwarding-hit-under-miss)
    - [ ] [5.2 Relation between memory coherency and consistency, their implementation on systems with shared bus and when multiple rings topologies are used, MESI, MOESI, home directory.](#52-relation-between-memory-coherency-and-consistency-their-implementation-on-systems-with-shared-bus-and-when-multiple-rings-topologies-are-used-mesi-moesi-home-directory)
    - [ ] [5.3 Rules for execution synchronization and data exchange in multiprocessor systems, mutex implementation, relation to consistency models and mechanisms to achieve expected algorithms behavior on systems with relaxed consistency models (PRAM, PSO, TSO, PC, barrier instructions).](#53-rules-for-execution-synchronization-and-data-exchange-in-multiprocessor-systems-mutex-implementation-relation-to-consistency-models-and-mechanisms-to-achieve-expected-algorithms-behavior-on-systems-with-relaxed-consistency-models-pram-pso-tso-pc-barrier-instructions)
    - [ ] [5.4 SMP and NUMA nodes interconnections networks, conflicts and rearrangeable networks, Beneš network.](#54-smp-and-numa-nodes-interconnections-networks-conflicts-and-rearrangeable-networks-beneš-network)
    - [ ] [5.5 Parallel computations on multiprocessor systems, OpenMP on NUMA and MPI on distributed memory systems, their combinations.](#55-parallel-computations-on-multiprocessor-systems-openmp-on-numa-and-mpi-on-distributed-memory-systems-their-combinations)
  - [6. KRP - I/O and network interfaces of computer and embedded systems, hardware and software implementation.](#6-krp---io-and-network-interfaces-of-computer-and-embedded-systems-hardware-and-software-implementation)
    - [ ] [6.1 USB I/O subsystem, structure and functionality of elements, protocol stack,transfer - transaction - packet hierarchy, transfer types and pipes, bandwidth allocation principles, enumeration process and PnP, descriptor hierarchy, USB device implementation.](#61-usb-io-subsystem-structure-and-functionality-of-elements-protocol-stacktransfer---transaction---packet-hierarchy-transfer-types-and-pipes-bandwidth-allocation-principles-enumeration-process-and-pnp-descriptor-hierarchy-usb-device-implementation)
    - [ ] [6.2 PCI Express (PCI) I/O subsystems, basic differences and commons of PCI and PCIe, protocol stack, transaction types, packet routing principles, quality of service support, PnP and enumeration process.](#62-pci-express-pci-io-subsystems-basic-differences-and-commons-of-pci-and-pcie-protocol-stack-transaction-types-packet-routing-principles-quality-of-service-support-pnp-and-enumeration-process)
    - [ ] [6.3 Ethernet based networking, VLAN, precision time protocol (PTP), stream reservation protocol (SRP), time sensitive networks (TSN).](#63-ethernet-based-networking-vlan-precision-time-protocol-ptp-stream-reservation-protocol-srp-time-sensitive-networks-tsn)
    - [ ] [6.4 In-vehicle networking, Controller Area Network (CAN, CAN-FD), Local Interconnect Network (LIN), FlexRay, data-link layer algorithms, physical topology constraints and relation to system design.](#64-in-vehicle-networking-controller-area-network-can-can-fd-local-interconnect-network-lin-flexray-data-link-layer-algorithms-physical-topology-constraints-and-relation-to-system-design)
  - [7. AVS - ARM based microcontrollers and signal processors; their functionality. Design and implementation of embedded systems for typical application areas.](#7-avs---arm-based-microcontrollers-and-signal-processors-their-functionality-design-and-implementation-of-embedded-systems-for-typical-application-areas)
    - [x] [7.1 Typical architecture and main features of ARM based microcontrollers. AMBA. I/O pin configuration. Common used peripheral circuits (I/O ports, timers, DMA controllers, NVIC controller, JTAG, SWD, A/D converters, D/A converters, SPI controllers, I2C controllers, UART, FLASH and SRAM memory).](#71-typical-architecture-and-main-features-of-arm-based-microcontrollers-amba-io-pin-configuration-common-used-peripheral-circuits-io-ports-timers-dma-controllers-nvic-controller-jtag-swd-ad-converters-da-converters-spi-controllers-i2c-controllers-uart-flash-and-sram-memory)
    - [x] [7.2 Typical architecture and main features of digital signal processors (DSP). Common used peripheral circuits. Special computational units and their features (ALU, MAC, SHIFT BARREL register, DAG).](#72-typical-architecture-and-main-features-of-digital-signal-processors-dsp-common-used-peripheral-circuits-special-computational-units-and-their-features-alu-mac-shift-barrel-register-dag)
    - [ ] [7.3 Digital signal processing: signal spectrum analysis (DFT, IDFT), correlation functions and their typical use, digital filters (FIR, IIR), signal interpolation, signal decimation.](#73-digital-signal-processing-signal-spectrum-analysis-dft-idft-correlation-functions-and-their-typical-use-digital-filters-fir-iir-signal-interpolation-signal-decimation)
    - [x] [7.4 Types of A/D converters. Sampling theorem. Anti-aliasing filter (AAF). Direct digital synthesis (DDS).](#74-types-of-ad-converters-sampling-theorem-anti-aliasing-filter-aaf-direct-digital-synthesis-dds)
    - [x] [7.5 User controls interfacing to microcontrollers (buttons, rotary encoders, graphic LCD, audio codecs, power switches, relays, contactors). Motion control (brush DC motor, stepper motor and brushless DC motor control).](#75-user-controls-interfacing-to-microcontrollers-buttons-rotary-encoders-graphic-lcd-audio-codecs-power-switches-relays-contactors-motion-control-brush-dc-motor-stepper-motor-and-brushless-dc-motor-control)
  - [8. PAG - Properties of parallel and distributed algorithms. Communication operations for parallel algorithms. Parallel algorithms for linear algebra. BE4M35PAG (Course web pages)](#8-pag---properties-of-parallel-and-distributed-algorithms-communication-operations-for-parallel-algorithms-parallel-algorithms-for-linear-algebra-be4m35pag-course-web-pages)
    - [x] [8.1 Describe basic communication operations used in parallel algorithms. Show cost analysis of one-to-all broadcast, all-to-all-broadcast, scatter, and all-to-all personalized communication on a ring, mesh, and hypercube. Describe All-Reduce and Prefix-Sum operations and outline their usage.](#81-describe-basic-communication-operations-used-in-parallel-algorithms-show-cost-analysis-of-one-to-all-broadcast-all-to-all-broadcast-scatter-and-all-to-all-personalized-communication-on-a-ring-mesh-and-hypercube-describe-all-reduce-and-prefix-sum-operations-and-outline-their-usage)
    - [x] [8.2 Describe performance metrics and scalability for parallel systems. How efficiency of a parallel algorithm depends on the problem size and the number of processors? Derive isoefficiency functions of a parallel algorithm for adding numbers (including communication between processors) and explain how it characterizes the algorithm.](#82-describe-performance-metrics-and-scalability-for-parallel-systems-how-efficiency-of-a-parallel-algorithm-depends-on-the-problem-size-and-the-number-of-processors-derive-isoefficiency-functions-of-a-parallel-algorithm-for-adding-numbers-including-communication-between-processors-and-explain-how-it-characterizes-the-algorithm)
    - [x] [8.3 Explain and compare two parallel algorithms for matrix-vector multiplication. Describe a parallel algorithm for matrix-matrix multiplication and explain the idea of Cannon’s algorithm. Discuss the principle and properties of the DNS algorithm used for matrix-matrix multiplication.](#83-explain-and-compare-two-parallel-algorithms-for-matrix-vector-multiplication-describe-a-parallel-algorithm-for-matrix-matrix-multiplication-and-explain-the-idea-of-cannons-algorithm-discuss-the-principle-and-properties-of-the-dns-algorithm-used-for-matrix-matrix-multiplication)
    - [x] [8.4 Outline the principle of sorting networks and describe parallel bitonic sort, including its scalability. Explain parallel enumeration sort algorithm on PRAM model, including its scalability.](#84-outline-the-principle-of-sorting-networks-and-describe-parallel-bitonic-sort-including-its-scalability-explain-parallel-enumeration-sort-algorithm-on-pram-model-including-its-scalability)
    - [x] [8.5 Explain all steps of a parallel algorithm for finding connected components in a graph given by the adjacency matrix. Using an example, illustrate a parallel algorithm for finding a maximal independent set in a sparse graph.](#85-explain-all-steps-of-a-parallel-algorithm-for-finding-connected-components-in-a-graph-given-by-the-adjacency-matrix-using-an-example-illustrate-a-parallel-algorithm-for-finding-a-maximal-independent-set-in-a-sparse-graph)
  - [9. ESW - Effective algorithms and optimization methods. Data structures, synchronization and multithreaded programs.](#9-esw---effective-algorithms-and-optimization-methods-data-structures-synchronization-and-multithreaded-programs)
    - [ ] [9.1 Java Virtual Machine, memory layout, frame, stack-oriented machine processing, ordinary object pointer, compressed ordinary object pointer. JVM bytecode, Just-in-time compiler, tired compilation, on-stack replacement, disassembler, decompiler. Global and local safe point, time to safe point. Automatic memory Management, generational hypothesis, garbage collectors. CPU and memory profiling, sampling and tracing approach, warm-up phase.](#91-java-virtual-machine-memory-layout-frame-stack-oriented-machine-processing-ordinary-object-pointer-compressed-ordinary-object-pointer-jvm-bytecode-just-in-time-compiler-tired-compilation-on-stack-replacement-disassembler-decompiler-global-and-local-safe-point-time-to-safe-point-automatic-memory-management-generational-hypothesis-garbage-collectors-cpu-and-memory-profiling-sampling-and-tracing-approach-warm-up-phase)
    - [ ] [9.2 Data races, CPU pipelining and superscalar architecture, memory barrier, volatile variable.  Synchronization - thin, fat and biased locking, reentrant locks. Atomic operations based on compare-and-set instructions, atomic field updaters. Non-blocking algorithms, wait free algorithms, non-blocking stack (LIFO).](#92-data-races-cpu-pipelining-and-superscalar-architecture-memory-barrier-volatile-variable--synchronization---thin-fat-and-biased-locking-reentrant-locks-atomic-operations-based-on-compare-and-set-instructions-atomic-field-updaters-non-blocking-algorithms-wait-free-algorithms-non-blocking-stack-lifo)
    - [ ] [9.3 Static and dynamic memory analysis, shallow and retained size, memory leak. Data Structures, Java primitives and objects, auto-boxing and unboxing, memory efficiency of complex data structures. Collection for performance, type specific collections, open addressing hashing, collision resolution schemes. Bloom filters, complexity, false positives, bloom filter extensions. Reference types - weak, soft, phantom.](#93-static-and-dynamic-memory-analysis-shallow-and-retained-size-memory-leak-data-structures-java-primitives-and-objects-auto-boxing-and-unboxing-memory-efficiency-of-complex-data-structures-collection-for-performance-type-specific-collections-open-addressing-hashing-collision-resolution-schemes-bloom-filters-complexity-false-positives-bloom-filter-extensions-reference-types---weak-soft-phantom)
    - [ ] [9.4 JVM object allocation, thread-local allocation buffers, object escape analysis, data locality, non-uniform memory allocation.](#94-jvm-object-allocation-thread-local-allocation-buffers-object-escape-analysis-data-locality-non-uniform-memory-allocation)
    - [ ] [9.5 Networking, OSI model, C10K problem. Blocking and non-blocking input/output, threading server, event-driven server. Event-based input/output approaches. Native buffers in JVM, channels and selectors.](#95-networking-osi-model-c10k-problem-blocking-and-non-blocking-inputoutput-threading-server-event-driven-server-event-based-inputoutput-approaches-native-buffers-in-jvm-channels-and-selectors)
    - [ ] [9.6 Synchronization in multi-threaded programs (atomic operations, mutex, semaphore, rw-lock, spinlock, RCU). When to use which mechanism? Performance bottlenecks of the mentioned mechanisms. Synchronization in “read-mostly workloads”, advantages and disadvantages of different synchronization mechanisms.](#96-synchronization-in-multi-threaded-programs-atomic-operations-mutex-semaphore-rw-lock-spinlock-rcu-when-to-use-which-mechanism-performance-bottlenecks-of-the-mentioned-mechanisms-synchronization-in-read-mostly-workloads-advantages-and-disadvantages-of-different-synchronization-mechanisms)
    - [ ] [9.7 Cache-efficient data structures and algorithms (e.g., matrix multiplication). Principles of cache memories, different kinds of cache misses. Self-evicting code, false sharing – what is it and how deal with it?](#97-cache-efficient-data-structures-and-algorithms-eg-matrix-multiplication-principles-of-cache-memories-different-kinds-of-cache-misses-self-evicting-code-false-sharing--what-is-it-and-how-deal-with-it)
    - [ ] [9.8 Profiling and optimizations of programs in compiled languages (e.g., C/C++). Hardware performance counters, profile-guided optimization. Basics of C/C++ compilers, AST, intermediate representation, high-level and low-level optimization passes.](#98-profiling-and-optimizations-of-programs-in-compiled-languages-eg-cc-hardware-performance-counters-profile-guided-optimization-basics-of-cc-compilers-ast-intermediate-representation-high-level-and-low-level-optimization-passes)

## 1. PAL - Polynomial algorithms for standard graph problems. Combinatorial and number-theoretical algorithms, isomorphism, prime numbers. Search trees and their use. Text search based on finite automata.

### 1.1 Notation of asymptotic complexity of algorithms. Basic notation of graph problems - degree, path, circuit, cycle. Graph representations by adjacency, distance, Laplacian and incidence matrices. Adjacency list representation.

**Notation of asymptotic complexity of algorithms**

There are three main notations for asymptotic complexity. 
1) Asymptotic upper bound $f(x) \in O(g(x))$ if the value of the function f is on or below the value of the function g.
2) Asymptotic lower bound $f(x)  \in \Omega(g(x))$ if the value of the function f is on or above the value of the function g.
3) Asymptotic tight bound $f(x) \in \Theta(g(x))$ if the value of the function f is equal to the value of the function g.

More in depth described in [TAL 2.1](#21-asymptotic-growth-of-functions-time-and-space-complexity-of-algorithms-correctness-of-algorithms---variant-and-invariant)

**Basic notation of graph problems**
- **Degree** is the property of the vertex. It symbolizes, how many edges are incident (are connected) to a given node. For directed graph these edges are further divided into **indegree** ($deg^+$) and **outdegree** ($deg^-$). The indegree symbolizes the number of edges, that have the node as destination, and outdegree as starting-point.

Degree - Undirected| Degree - Directed|
|:-:|:-:|
![Degree Undirected Graph](img/PAL_undirected_degree.png)|![Degree Directed Graph](img/PAL_degree_directed_graph.png)

Handshaking lemma: sum of all degrees in graph is 2x number of Edges. 

$$\sum_{v\in V} deg(v) = 2|E|$$

**Complete Graph** - graph where every node is directry connected to all other nodes.

**Path** - sequence of vertices and edges, where all **vertices differ** from each other. Hamiltonian path visites every vertex. Hamiltonian cycle in addition to that starts and ends in the same node.

**Trail** - sequence of vertices and edges, where all **edges differ** from each other. Eulerian trail visites every edge exactly once (domeček jedním tahem). Eulerian circuit starts and ends in the same node.

**Circuit** - closed path (starts and ends in the same vertex)

**Cycle** - the cycle is the same as circuit, but the edges and verteces can repeat.

**Connected graph** is graph where exists path between all verteces.

**Three** is connected graph, where every two vertices are connected by just one path. 

**Adjecency matrix** is matrix, where $a_{i,j} = \begin{cases} 1& \text{for } \{v_i, v_j\} \in E \\ 0& \text{otherwise}\end{cases}$. The *1* in the field *i,j* symbolises weather the nodes *i* and *j* are connected.

**Laplacian matrix** is matrix where $l_{i,j} = \begin{cases} deg(v_i)& \text{for } i=j\\ -1& \text{for } \{v_i, v_j\} \in E\\ 0& \text{otherwise}\end{cases}$. On the diagonal are degrees of the nodes, and each pair of nodes *i,j* that is connected is represented by -1 in the matrix. Note, that sum of row, or column is always 0.

**Distance matrix** is simmiral to adrecency matrix, but instead the plain 1, there is cost of the edge insead.

**Incidence matrix** the matrix has $V\times N$ dimensions. Where each column symbolizes each vertex, in each column there is -1 and 1. The -1 symbolizes start node of the edge and 1 symbolizes the end node of the edge. $(I)_{i,j}=\begin{cases} -1& \text{for } e_j = (v_i, *)\\ +1& \text{for } e_j = (*, v_i)\\ 0& \text{otherwise}\end{cases}$

**Adjacency list** (list of neighbours) - each node keeps track of the neighbouring nodes. 

For sparse graphs the adjacency list is usually faster. Only good thing about matricies is when we want to remove edge, it is pretty easy.

### 1.2 Algorithms for minimum spanning tree (Prim-Jarník, Kruskal, Borůvka), strongly connected components (Kosaraju-Sharir, Tarjan), Euler trail. Union-find problem. Graph isomorphism, tree isomorphism.

### 1.3 Generation and enumeration of combinatorial objects - subsets, k-element subsets, permutations. Gray codes. Prime numbers, sieve of Eratosthenes. Pseudorandom numbers properties. Linear congruential generator.

### 1.4 Search trees - data structures, operations, and their complexities. Binary tree, AVL tree, red-black tree (RB-tree), B-tree and B+ tree, splay tree, k-d tree. Nearest neighbor searching in k-d trees. Skip list.
 
### 1.5 Finite automata, regular expressions, operations over regular languages. Bit representation of nondeterministic finite automata. Text search algorithms - exact pattern matching, approximate pattern matching (Hamming and Levenshtein distance), dictionary automata.

## 2. TAL - Problem/language complexity classes with respect to the time complexity of their solution and memory complexity including undecidable problems/languages.

### 2.1 Asymptotic growth of functions, time and space complexity of algorithms. Correctness of algorithms - variant and invariant.

### 2.2 Deterministic Turing machines, multitape Turing machines, and Nondeterministic Turing machines.

### 2.3 Decision problems and languages. Complexity classes P, NP, co-NP. Reduction and polynomial reduction, class NPC. Cook theorem. Heuristics and approximate algorithms for solving NP complete problems.

### 2.4 Classes based on space complexity: PSPACE and NPSPACE. Savitch Theorem.

### 2.5 Randomized algorithms. Randomized Turing machines. Classes based on randomization: RP, ZPP, co-RP.

### 2.6 Decidability and undecidability. Recursive and recursively enumerable languages. Diagonal language. Universal language and Universal Turing machine.

## 3. KO - Combinatorial optimization problems - formulation, complexity analysis, algorithms and example applications. 

### 3.1 Integer Linear Programming. Shortest paths problem and traveling salesman problem ILP formulations. Branch and Bound algorithm. Problem formulations using ILP. Special ILP problems solvable in polynomial time.

### 3.2 Shortest paths problem. Dijkstra, Bellman-Ford, and Floyd–Warshall algorithms. Shortest paths in directed acyclic graphs. Problem formulations using shortest paths.

### 3.3 Network flows. Maximum flow and minimum cut problems. Ford-Fulkerson algorithm.  Feasible flow with balances. Minimum cost flow and cycle-canceling algorithm. Problem formulations using network flows. Maximum cardinality matching.

### 3.4 Knapsack problem. Approximation algorithm, dynamic programming approach, approximation scheme.

### 3.5 Traveling salesman problem. Double-tree algorithm and Christofides algorithm for the metric problem. Local search k-OPT.

### 3.6 Scheduling - problem description and notation. One resource - Bratley algorithm, Horn algorithm. Parallel identical resources - list scheduling, dynamic programming. Project scheduling with temporal constraints - relative order and time-indexed ILP formulations.

### 3.7 Constraint Satisfaction Problem. AC3 algorithm.

## 4. ISC - Design and implementation of in-chip integrated systems, application specific systems.

### 4.1 Main features and economical aspects of the Application specific integrated circuits systems: full custom design, gate array, standard cells, programmable array logic;

### 4.2 Design principles of mix-signal integrated circuits, purpose of hierarchical design, digital and analogue block interface, CAD design tools for automatic circuit generation; functional and static time analysis, formal verification; Verilog-A, Verilog-AMS, VHDL-A.

### 4.3 Front end design - functional specification, RTL, logic synthesis, Gate-level netlist, behavioral stimulus extraction.

### 4.4 Back End design - specification of Design Kit, Floorplanning, place and route, layout, parasitic extraction, layout versus schema check (LVS).

### 4.5 Tape out and IC fabrication process, integrated systems verification, scaling and design mapping to different technologies.

## 5. PAP - Advanced architectures of processors, memory and peripheral circuits and multiprocessor computers. 

### 5.1 Superscalar techniques used in nodes of multiprocessor systems, data flow inside the processor, Tomasulo algorithm and its deficiencies, precise exceptions support, architectural state, register renaming, reservation station, reorder buffer, instruction fetch, decode, dispatch, issue, execute, finish, complete, reorder, branch prediction, store forwarding, hit under miss.

**Data flow inside the processor**

Stages of the pipeline:
- **IF** instruction fetch - load instruction from the instruction memory. 
- **ID** instruction decode - translate the instruction into correct signals for the ALU and other units, load corresponging registers
- **EX** execute - execute the instruction like add two numbers
- **MEM** memory access - write/load to/from memory.
- **WB** write the result back to the register or other storage location.

not all instruction go trough all stages. Like Branch, which does not go to mem and write-back phase

Diversified pipelining - multiple executin units

Dynamic pipelining - out of order execution

![Dynamic pipelining](img/PAP_dynamic_pipelining.png)

Dispatch buffer - the buffer where the prepared instructions await for the right execution unit to be available

Reorder buffer - the buffer, where the out-of-order instructions are put back in the correct order. Needed to support precise exceptions (complete only the instructions that happened before the exception)

The buffers can be added between all stages to support out-of-order execution on more fronts. They all need to be tracked to be correcly positoined in completion buffer.

Superscalar organisation:
- **Fetch** - the pipeline has width W (number of parallel pipelines). Fetch able to load W instructions from instruction Cache in every cycle. I-cache must be wide enough to store W instructions, that can be accessed at the same time. The loading can be degraded by unaligned instructions and jump instructions (we load not from the beginning of the line)
Solution: Static(Compilator chooses best placement) Dynamic(Solved by Hardware during runtime)\
The un-alignment can be solved by T-Logic - two way associative cache.

![TFETCH](img/PAP_T_LOGIC.png)

- **Decode** - the complexity is wildly different when decoding CISC/RISC instructions. Tasks:
  - indetify the infividual instructions (and even lengths for CISC)  
  - determine instruction types
  - detect inner dependencies, determine set of independent instructions to dispatch to next stages\
can contain Predecoding - if i-cache misses the instructions while loading from memory to the Cache are partialy decoded, to be searched for the potential jumps and identifiaction of independant instructions, which then simplyfies the Decode Process.
- **Dispatch** - route the insturctions to the corresponding functional units for execution. The operand values might not be ready for some instructions, solved by stall, or using reservation stations (buffers). We distinguish:
  - Centralized reservation stations
  - Distributed reservation stations
  - Hybrid reservation stations (or clustered)\
  Dispatching - assign the functional unit for execution to the instruction
  Issue - initialize execution of the instruction

Centralized reservation station|  Distributed reservation station
:-------------------------:|:-------------------------:
![Centralized reservation station](img/PAP_centralized_reservation_station.png)  |  ![Distributed reservation station](img/PAP_distributed_reservation_station.png)

- **Execute** - usually more execution units, than the width of the pipeline. Current trend: more diversified pipelines (in past only Integer operations and Floating point operations were the major ones). More execution units - more complex hardware - need the connections to connect hazard unit to all the execution units for forewarding, reservation stations need monitor for the availability(ready state) or ready operand values(tag matching)\
Best mix of functional units? It depends, usually 2:1:2 ALU:Branch:Load/Store
- **Complete** - instruction is completed, when it finishes execution and updates the machine state. It is when it exits execution unit and enters completion buffer. It can wait another cycles, before it is retired.
- **Retire** - The instruction is retired, when it leaves the completion buffer and updates Data-Cache. 

**Precise exception** support requieres reorder/completion buffer. The Retire of the instructions need to happen in the program order. If the exception occurs, the instruction is tagged. When a tagged instruction is detected, all preceding instructions need to be completed, the the error instruction is not completed and preceding processor state is saved. The following instructions and progress in pipelines is discarded.

![Completion and reorder buffer](img/PAP_completion_reorder_buffer.png)

**Tomasulo algorithm**



### 5.2 Relation between memory coherency and consistency, their implementation on systems with shared bus and when multiple rings topologies are used, MESI, MOESI, home directory.

### 5.3 Rules for execution synchronization and data exchange in multiprocessor systems, mutex implementation, relation to consistency models and mechanisms to achieve expected algorithms behavior on systems with relaxed consistency models (PRAM, PSO, TSO, PC, barrier instructions).

### 5.4 SMP and NUMA nodes interconnections networks, conflicts and rearrangeable networks, Beneš network.

### 5.5 Parallel computations on multiprocessor systems, OpenMP on NUMA and MPI on distributed memory systems, their combinations.

## 6. KRP - I/O and network interfaces of computer and embedded systems, hardware and software implementation. 

### 6.1 USB I/O subsystem, structure and functionality of elements, protocol stack, transfer - transaction - packet hierarchy, transfer types and pipes, bandwidth allocation principles, enumeration process and PnP, descriptor hierarchy, USB device implementation.

### 6.2 PCI Express (PCI) I/O subsystems, basic differences and commons of PCI and PCIe, protocol stack, transaction types, packet routing principles, quality of service support, PnP and enumeration process.

### 6.3 Ethernet based networking, VLAN, precision time protocol (PTP), stream reservation protocol (SRP), time sensitive networks (TSN).

### 6.4 In-vehicle networking, Controller Area Network (CAN, CAN-FD), Local Interconnect Network (LIN), FlexRay, data-link layer algorithms, physical topology constraints and relation to system design.

## 7. AVS - ARM based microcontrollers and signal processors; their functionality. Design and implementation of embedded systems for typical application areas.

### 7.1 Typical architecture and main features of ARM based microcontrollers. AMBA. I/O pin configuration. Common used peripheral circuits (I/O ports, timers, DMA controllers, NVIC controller, JTAG, SWD, A/D converters, D/A converters, SPI controllers, I2C controllers, UART, FLASH and SRAM memory).

**AMBA** - *Advanced Microcontroller Bus Architecture* - ARM specific on-chip communication protocols. Different types of AMBA:
- AMBA AXI - *Advanced eXtensible Interface* - high-speed sub-micrometer interconnect. separate address/controll and data phases. Burst based transactions with only start address issued. Older iterations named as ASB - *Advanced system bus*
- AMBA AHB - *Advanced High-performance Bus* - two bus-cycle address phase and data phase. One bus-master at the same time. There is also AHBlight (Only one master in AHB light) [AHB vs AHBlight](http://www.vlsiip.com/amba/ahb/ahb_0002.html)
- AMBA APB - *Advanced Peripheral Bus* - low bandwith controll access, similar to the AHB, but much slower and much simpler communication (for example no bursts)

Source: [AMBA](https://en.wikipedia.org/wiki/Advanced_Microcontroller_Bus_Architecture)

In the stellar SR6P6 the buses are used followingly. The AXI NIX400 crossbar connects the R52 Cores in one cluster with local NVM and RAM. AHB is used to to connect Cortex M4 accelerators domains to their peripherals. NUC can be connected to with either AXI, AHB, AHBLight. The APB is used in peripheral bridge.

If you want to use certain periphery you need to start the corresponding BUS. (which is kinda obvious, but just mentioning it...)

**I/O pin configuration**

GPIO Schema |  Register configuration
:-------------------------:|:-------------------------:
![GPIO Configuration](img/AVS_gpio_structure.png) |  ![Configuration](img/AVS_configuration.png)

- Mode - Output/Input/Alternate Function/Analog - Alternate function can be UART output, or the TIMER output.
- Push-Pull / OpenDrain - Push-Pull can both provide current and sink current, the OpenDrain only sinks current. 
- Pull-Up / Pull-Down / Floating - Set what is the default state of the pin.

**Timers**

There can be different timers - 16bit, 32bit, HighResolutionTimers (Good for switch-mode power supply)

Useful for PWM, measuring time, Generating interrupts, connecting RotaryEncored, Counting triggers. Capture/Compare and others.

**DMA - direct memory access**

Module used to handle memory manipulation. Can be Periphery-to-Mem, Mem-to-periphery, Mem-to-Mem (And even Periphery to Periphery, but that is special version of the Priphery-to-Mem)

**NVIC - nested vector interrupt controller**

Handles interrupts to the Microcontroller. Enables multiple interrupts to be handeled with the different priority and masking them. There is (as far as I know one unmaskmable interrupt). It is possible to jump to the higher priority interrupt, while executing lower priority interrupt. When interrupt occurs, the processor saves the state of the current routine and executes the higher priority one.

**JTAG + SWD**

Debugging interface standards. 

Jtag requieres 4 signal lines. 

The SWD uses 2-pin interface, that uses the same protocol. Debugger becomes another AMBA busmaster.

Source: [SWJ vs JTAG debugging](https://electronics.stackexchange.com/questions/53571/jtag-vs-swd-debugging)

**A/D + D/A** 

A/D - Analog to Digital converter. The base of the A/D is sampling the analog signal at regular intervals and converting this signal to digital value. About types and the sampling refer to the [7.4 Types of A/D](#74-types-of-ad-converters-sampling-theorem-anti-aliasing-filter-aaf-direct-digital-synthesis-dds). 

D/A - Digital to Analog converter. The base of the D/A convertor in converting the digital value into the Analog signal. 
More info in [7.4 Types of A/D](#74-types-of-ad-converters-sampling-theorem-anti-aliasing-filter-aaf-direct-digital-synthesis-dds). 

**SPI** 

Uses four signals to communicate. Clock, Chip select, MOSI (Master out Slave in), MISO (Master in Slave out). 

Chip select in 0 - communication active. Different SPI modes. Changing the idle state and on which edge (rising/falling) the data will be sampled. Supports FULL-Duplex communication and mupliple slaves by daisychaining them.

**I2C**

Bidirectional, but only Half-Duplex. Master-Slave communication and the bus is Synchronous - clocked with clock signal.
Also can be called TWI (Two-wire interface). One wire is clock, while the other is data. SDA - serial data, SCL - serial clock.

![I2C Frame](img/AVS_I2C_FRAME.png)

The communication is acknowledged - after every data the receiver acks the received data.

**UART/USART**

UART - universal asynchronous receiver/transmitter. Half-duplex. More simple than USART (Synchronous/asynchronous). Synchronous communication requieres clock signal and that means the communication is a more reliable and can be faster.

UART frame - databits (7,8,9), Optional Parity bit and number of stop bits (1, 1.5, 2)

**FLASH + SRAM**

Flash is non-volatile memory (NVM). Different types, based on the type of manufacturing NOR-FLASH, NAND-FLASH, EEPROM...
NOR-FLASH good random access - good for code storage in micro. NAND flash in consumer electronics, like flash drives. EEPROM electrically erasable programable memory - stable, ideal for external sotrrage for embedded devices.

SRAM (Static Random Access Memory) is a volatile memory. More expensive and faster, than FLASH memory. Used for Cache and TCM. Cana be Synchronous of Asynchronous. Tha fastest RAMs, like in desktop PCs are Synchronous. 

### 7.2 Typical architecture and main features of digital signal processors (DSP). Common used peripheral circuits. Special computational units and their features (ALU, MAC, SHIFT BARREL register, DAG).

DSPs are designed expecially  to efficiently process digital signals in real-time. They are used for application that requiere a lot of data processing, like audio, video, telecommunication, radars...

They are ussualy used with ADC and DAC. Analog data to ADC -> into DSP, carry out some mathematical operations -> DAC -> back to analog signal.

The common peripheral circuits are simillar to the general microcontrollers - gpio, ADC, DAC, Timers, interrupt controllers, power management, DMAs...

DSP typically has Harward architecture - the Instruction and Data memory are separated, as well as the ALU. Has limited instruction set (RISC core). Optimized for dataflow processing directly supported by hardware: circular buffers (FIR), bit reversal (FFT), special instructions for SIMD. Special ALU and MAC units (MAC=Multiply and accumulate)

**ALU** has usual operations (Add sub, div, ++, --, abs, and logical operations AND, OR, XOR...).

**MAC** high speed multiply and accumulate. Multiplication is very important oparatoin in DSP. can have fractional mode 1.15 or integer mode 16.0 (for 16-bit operations). 

**DAG** (Data adress generator) - HW support for circular buffers. 

**Shift barrel register** provides arithmetical and logical shifts. Normalization and even some derivations. 

All the components are highly specialized for the targeted market (Autio, Telecommunication, Sience...)

### 7.3 Digital signal processing: signal spectrum analysis (DFT, IDFT), correlation functions and their typical use, digital filters (FIR, IIR), signal interpolation, signal decimation.

**Impulse characteristic** - system response to the Dirac function

**Transfer characterictic** - system response to the unit step

**DFT**

**IDFT**

### 7.4 Types of A/D converters. Sampling theorem. Anti-aliasing filter (AAF). Direct digital synthesis (DDS).

**Types of A/D converters**
- **Succesive approximation register (SAR) ADC** - defacto binary search. We sample the $V_{in}$. And set the highest bit in DAC to 1. If the $V_{dac}$ is higher than *in* the bit is set to 0, otherwise it stays 1. Move to the less significant bit. This way we will create the value on our DAC, that is closest to the $V_{in}$.\
Intermediete in speed and accuracy.
- **Integrating ADC or Dual slope ADC** - high resolution 12-18 bit. Good accuracy, high stability, low cost. Low sampling rate around 10samples/s. Suitable for slowly changing measurements, like temperature.\
The functionality is described in picture. First the $S_1$ is closed - the charge on the capacitor *C* is zero. Then the $S_2$ is closed for the number of clock cycles N resulting in charge in C. Then the $S_3$ is closed integrating the voltage back to zero, using the negative reference voltage. after time x the charge is 0. Now, knowing the time x, number of clock cycles N and reference voltage, we can count the $V_{in}$ 
- **Paralel ADC or Flash converter** - Like thermometer. Not suitable for high resolution ADCs, but it is very fast. 
- **Charge balancing ADC** - charge balancing integrator. Voltage is translated to the frequency. The pulse is generated when integrated Voltage reaches certain treshold and integrated charge is discharged (depicted as $V_{pulse}$). Number of pulses are counted and that is then translated into the Voltage.\
Input voltage should be lowed than $V_{ref}/2$
- **Sigma-Delta** - engeneer is buying coffee. Based on the oversampling principle. Making many errors and then taking average. 

Synchronous data transfer is usually easier to handle than asynchronous, but it is dependant on the clock frequency - generates jitter. 

SAR DAC | Integrating ADC
|:---:|:--:|
![SAR ADC](img/AVS_SAR_ADC.png)|![Integrating ADC](img/AVS_integrating_ADC.png)

charge balancing | |
|:---:|:--:|
![Charge balancing](img/AVS_charge_balancing.png)|![Charge Balancing schema](img/AVS_charge_balancing_schema.png)

Sigma-delta | Paralel ADC |
|:---:|:--:|
![Sigma Delta](img/AVS_sigma_delta_basics.png)|![Paralel ADC](img/AVS_parallel_adc.png)

Source [Integrating and charge balancing](https://www.youtube.com/watch?v=f-6shAZL4Ak), [Sigma-Delta](https://www.youtube.com/watch?v=M5Vx-X66seg), [Parallel and SAR](https://www.youtube.com/watch?v=75GcoQ9_LFI)

**Anti-aliasing filter** - also a low-pass filter. When the sampling frequency is lower, than the frequency of the signal we are sampling, we can get garbage. It is important to know this and discard the high frequencies we are not able to reliably capture. For this we use low-pass filter. Only the low frequencies, we know we can represent are sampled, and high frequencies are ignored.
Aliasing |
|:---:|
![aliasing](img/AVS_aliasing.png)|

**Sampling (Shannon-Kotelnik or Nyquist-Shannon) theorem** - Reverse and accurate reconstruction of a continuous frequency signal from discrete values is only possible if the sampling frequency is at least twice higher than the maximum frequency of the reconstructed signal. 

$$
f_{s} > 2f_{sig\_max}
$$
where $f_s$ is sampling frequency and $f_{sig_max}$ is maximum frequency of the signal we try to capture. Nyquist frequency is the $f_{s} \over 2$. Same thing, but isnted of the sampling we tank about frequency.

**DDS - Direct Digital Synthesis** - used to represent some signal stored in memory.

DDS with acumulator rounding|
|:-:|
![Acumulator Rrounding](img/AVS_DDS_se_zaokrouhlovanim.png)|

Register specifies the frequency at which the output signal will be updated. If set to 1 it will be the same as the Reference clock. Acumulator is just sum, that owerflows, once the *Acumulator>size(Lookup_table)* The Converter phase-amplitude is just fancy way of saying lookup table. The acumulator to retrieve value from lookup table and display it on the D/A. 

Difference between rounding and not-rounding DDS is that only the high bits are used to adress the data in lookup table (depends in the size of the table). It is ideal to put low-pass filter after the D/A converter.

One need to keep in mind the Nyquist frequency - the frequency of the outputed signal needs to be lower, than the $f_{clk} \over 2$ but that ususally spawns other frequencies (higher harmonic frequencies) and it is recommend to keep the output frequency at 40%. That being $40\% {f_{clk} \over 2}$

### 7.5 User controls interfacing to microcontrollers (buttons, rotary encoders, graphic LCD, audio codecs, power switches, relays, contactors). Motion control (brush DC motor, stepper motor and brushless DC motor control).

**Button** - can be pulled-up or down, but needs to be pulled somewhere. Debouncing needs to be solved. Debouncing can be solved on the Hardware or Software level.

On Hardware we can add schmitt trigger (Histeresy) or by implementing RC circuit (low-pass filter)

In Software, we can count the occurences of the HIGH/LOW signal, and we can take the definitive answer after encountering N continuous samples. We can also wait after the pressing and unpressing the button and then checking again.

**Rotary-encoder** - encoder can be incremental or absolute - incremental only informs us about the direction of the encoder, while absolute (using usually gray code) specifies its absolute position.

The most popular is quadrature rotary encoder. Uses two signals to represent the motion. Microcontrollers have special timers that can easily interact with the rotary encoder.

**Graphic LCD** - graphical LCD can be simple like 7-segment display, or more complex LCD displays. The basic HD44780 usis data bus for selecting the segment and then writing to that segment. The segment can be chosen by special instructions, where we can move with the cursor to the left/right.

Usually the LCD is connected via shift-register or specialized controller connected to the Micro via SPI or IIC, to save pins.

**Audio codecs** - the most basic audio interaction from the Microcontroller is by using PWM signal connected to the Piezzo buzzer.

The more complex audio requieres audio codec - single device usually connected via bus (SPI, I2C), that Encodes analog audio as digital signals and decodes digital audio back to analogsing  using ADC and DAC.

**Relays** - relay can serve as the isolation of two or more Domains, in each domain there can be different signal voltage. The relay enables this isolatoin by switching the contact using magnetic force generated on the coil. 

The relay can be in different instances - default off, default on. Or used as a switch with break first, then connect, or connect first, then break. They are called BBM and MBB (Break before Make and Make before Break)

The donwside of the relay is slow switching speed, and degradation due to the induction and high currents.

The micro is usually not powerfull enough to set relay - it might need external source and swith it using transistor.

The relay can also be semi-conductor based. Using IRED LED and light-sensitive element.

**Brush DC motor** - All motors are based on the magnetic field and motion created by that. The brush motor connect to the rotor using literal brushes, that concuct electricity to the part of the rotor. Around rotor, there are static magnets and field of the static magnets and the generated field try to align - creating motion. But since the brushes are not hard connected, they will stay still, not allowing the magnetic fields to align.

Same probles as with relay - induction and high current needed. Can controll the motor speed using PWM. 

For both way control we can use H-bridge.

**Stepper motor** - stepper motor uses coils around static magnet to fine tune the position of the rotor.

**Brushless DC motor controll** - simmilar to the brush DC motor, but the static magnet is rotating - no need of connection to the rotor trough brushes, but the controll is harder, since we need to know whet to send current trough the surrounding coils. Usually 3 coils connected trough half H-bridge. information about the position of the rotor can be obtained using HAL sensor.

Brush Motor | Stepper motor steps
|:-:|:-:|
![Brush motor](img/AVS_brush_motor.png)|![Stepper motor phases](img/AVS_Stepper_motor.png)

## 8. PAG - Properties of parallel and distributed algorithms. Communication operations for parallel algorithms. Parallel algorithms for linear algebra. BE4M35PAG (Course web pages)

### 8.1 Describe basic communication operations used in parallel algorithms. Show cost analysis of one-to-all broadcast, all-to-all-broadcast, scatter, and all-to-all personalized communication on a ring, mesh, and hypercube. Describe All-Reduce and Prefix-Sum operations and outline their usage.

**Main terms**

- NUMA - non unified memory access
- UMA - unified memory access
- SIMD - single instruction stream, multiple data streams
- MIMD - multiple instruction streams multiple data streams
- Diameter - distance between the farthest two nodes in network
- Bisection width - The minimum number of connections that need to be cut to devide network into two equel parts.
- Cost - total number of connections
- Arc Connectivity - the minimum number of arcs that must be removed from the network to beak it into two disconnected networks.

Evaluation of networks|
|:-:|
![Evaluation of networks](img/PAG_evalueation_of_networks.png)

The main compute is 
$$
t_{comm}=t_s + (mt_w+t_h)l
$$
where $t_{comm}$ is time needed for communication. $t_s$ is startup time $t_h$ is per-hop time and $t_w$ is word transfer time. $m$ is size of the message and $l$ is number of traversed communication links. 

Since $t_h$ is usually smaller than other variables, and $m$ is large, it is usually ignored. And due to usage of cut-trough routing the equation can be simplified to 

$$
t_{comm}=t_s+t_wm
$$

**one-to-all broadcast**

Cost analysis $T=(t_s+t_wm)\log p$

Due to cut-trough routing we can delegate sending the message to other nodes.

One-to-All broadcast ring | One-to-All broadcast 3D mesh |
|:-:|:-:|
![One-to-all broadcast ring](img/PAG_one_to_all_broadcast.png)|![One-to-all boradcast 3D mesh](img/PAG_Hybercube_alltoonebroadcast.png)| 

**All-to-all broadcast**

The same applies for all-to-all reduction, but in reverse

*Cost of all-to-all ring*:  $T=(t_s + t_wm)*(p-1)$

*Cost of all-to-all mesh*: 
1) phase on lines: $T=(t_s+t_wm)*(\sqrt{p}-1)$
2) phase on columns: $T=(t_s+t_wm\sqrt{p})*(\sqrt{p}-1)$

Sum of the phases: $T=2t_s+(\sqrt{p}-1)+t_wm(p-1)$

*Cost of all-to-all hypercube*: again it can be devided into phases to more understand the problem, but the final sum is: $T=\sum_{k=1}^{\log p} t_s + t_wm*2^{k-1}$ which can be simplified to the $T=t_s\log p + t_wm(p-1)$

All-to-All ring | All-to-All mesh | All-to-All hypercube | 
|:-:|:-:|:-:|
![all-to-all broadcast ring](img/PAG_all_to_all_ring.png)|![all-to-all broadcast mesh](img/PAG_all_to_all_mesh.png)|![all-to-all boradcast 3D mesh](img/PAG_all-to-all-hypercube.png)| 

**Scatter - Gather** -  Scatter: one node has personalized message for the all others. The time cost is the same as the All-to-all broadcast $T=t_s\log p + t_wm(p-1)$, but here it is the same for the linear array, as well as 2D mesh.

**All-reduce** - each node starts with buffer of size m, and at the end of the operation all nodes have identical buffers. The final buffer is created by combining all the buffers (eg vector sum)

Can be implemented as all-to-one reduction and one-to-all broadcast. Or more efficiently by all-to-all broadcast patter, but the size of the message is not increasing, which results in $T=(t_s+t_wm)\log p$

**Prefix-sum** - each node starts with the unique number. And at the end each node will have sum of the numbers from nodes, that have smaller ID than the Node.

Can be implemented using modified all-to-all broadcast, summing only the numbers with label smaller, than my ID. But I can still summ something for my future communication partner, like show in the picture, where node $0$ summed 0+1 for the node $2$.

Prefix sum|Scatter|
|:-:|:-:|
![Prefix-summ](img/PAG_prefix-sum.png)|![scatter](img/PAG_scatter.png)

**all-to-all personalized** - also called total exchange

*Cost on ring* - $T=\sum_{k=1}^{p-1}t_s+t_wm(p-k)$ after some cleaver magic it results into: $T=(t_s+t_wm{p\over 2})(p-1)$

*Cost on mesh* - same as cost on 2 rings of size $\sqrt{p}$ (but message size stays the same) therfore $T=(2t_s+t_wmp)(\sqrt{p}-1)$

*Cost on hypercube* - the same principle as with previous networks is not optimal - resolves in $T=(t_s+t_wm{p\over 2})\log p$ cost. The optimal algoritm cost is $T=t_wm(p-1)$

Total exchange Ring|Total exchange Mesh| Total exchange Hypercube
|:-:|:-:|:-:|
![total exchange ring](img/PAG_total_exchange_ring.png)|![total excahnge mesh](img/PAG_total_exchange_mesh.png)|![total exchange hypercube](img/PAG_total_exchange_hypercube.png)

### 8.2 Describe performance metrics and scalability for parallel systems. How efficiency of a parallel algorithm depends on the problem size and the number of processors? Derive isoefficiency functions of a parallel algorithm for adding numbers (including communication between processors) and explain how it characterizes the algorithm.

Problem of scalability - when we use x processors it does not mean the result will be x times faster. Communication overhead, interprocessor communication, deviding of the work, waiting for more work, and number of processors is dependant on the data (adding two numbers will be the same when using one or twenty processors)

In short - there is exess computation, that does not to be performed by serial version of the program.

The **Serial runtime** $T_s$ is time elapsed between start and end of the execution. The **Parallel runtime** $T_p$ is time between first processor starting and last processor ending the execution.

We can copmute the non-usefull work by computing overhead function $T_O$. The **overhead** function is given by $T_O=pT_p-T_s$ time spent in parralel multiplied by the number of processors and substracting the amount of time the serial version would need.

Benefit of the parallelism - the **Speedup** can be compuded by $S={T_s \over T_p}$. Represents Ration of how much faster is the problem computed on single proccessor to computed on p identical processors in parallel.

Speedup can be as low as 0 (parallel program never terminates). The Speedup is in theory bounded by p, but there can be Superlinear Speedup (paralle program does less work, then serial counterpart). Example: some special example of depth first search.

**Amdahl's law**: The program contains part that is naturally sequential - it cannot be speeded up by adding more processors. $T_p = \beta T_s + (1-\beta){T_s\over p}$ 
$$S \leqq {T_s \over \beta T_s + (1-\beta){T_s\over p} } = {p\over \beta p+(1-\beta)}$$ 

if the $p \to \infty$ then the limit is $1\over\beta$

**Efficiency** is a measure of the fraction of time for which the processor is usefully employed.

$$E = {S\over p}$$

**Cost** or work is the product of parallel runtime and the number of processing elements used 
$p \cdot T_p$
. Cost represents the **Sum** of time each processing element spends solving the problem. 

The parallel system is cost-optimal if the cost of solving a problem on a paralle computer is asysmptotically identical to serial cost.

Since 

$$E = {T_s \over p T_p}$$

, for cost optima systems $E=O(1)$

The E can be also rewritten as 

$$E={1 \over 1 + {T_o \over T_s}}$$

By adding the processors the $T_o$ goes up. 

**Isoefficiency** describes the rate at which the problem size mut increase with respect to the number of processing elements to keep efficiency fixed. This rate determnes the **Scalability** the slower the rate, the better. 

Lets define W as the aysmptotic number of operations associated with the best serail algorithm to solve the problem.

$$T_p = {W+ T_o(W,p) \over p}$$

The resulting speedup: 

$$S = {W\over T_p} = {W_p\over W+T_o(W,p)}$$

And efficiency:

$$E = {S\over p} = {W\over W+T_o(W,p)} = {1\over 1+T_o(W,p)/W}$$

If we modify the function:

$$W={E\over 1-E} T_o(W,p)$$

And if  $K = E/(1-E)$ is a constant function to be maintained we have:

$$W=KT_o(W,p)$$

The problem size W can usually be obtained as function of p. This function is called the **isoefficiency function**. This function determines the *ease* of maintaining constant efficiency. 

If the W needs to grow only linearly with respect to p then the parallel system is **highly scalable**

If we solve the isoeffitiency function for p=f(n) we get the maximum number of processors, that can be used to remain cost-effitient

**EXAMPLE OF ADDING NUMBERS**

Adding numbers $p = n$| Adding numbers $p \neq n$|
|:-:|:-:|
![Adding numers in parallel](img/PAG_adding_numbers_visualisation.png)|![Adding numbers cost optimal](img/PAG_adding_numbers.png)

The ${T_s = \Theta(n)}$ and using all to one reduction the $T_p = \Theta(\log n)$

The speedup $S={T_s\over T_p}={n\over \log n}$

The efficiency $E={S\over p} = {n\over p \log n }$ if the $p=n$ the $E={1\over \log n}$ the sollution is not cost-optimal

The cost = $pT_p = n \log n$ proving once again the solution is not cost-optimal, since $T_s = \Theta(n)$

It the $p \neq n$

$$T_p = n/p + t_w \log p$$

The cost is $p \cdot T_p = n+p\log p$ and as long as $n \in \Omega(p\log p)$ the computation is cost-optimal

The isoefficiency: $W=KT_o$ ; $T_o = pT_p-W = n+pt_w\log p -n = pt_w\log p$
That means the isoefficiency is 
$$\Theta(p\log p)$$



### 8.3 Explain and compare two parallel algorithms for matrix-vector multiplication. Describe a parallel algorithm for matrix-matrix multiplication and explain the idea of Cannon’s algorithm. Discuss the principle and properties of the DNS algorithm used for matrix-matrix multiplication.

**Matrix x vector multiplication**

either devide by rows or devide by 2D chunks

1D - all-to-all bradcast and then compute
2D - one-to-all broadcast over file compute and then acummulate.

2D can use more processors, 1D has better iso-effitiency

TODO!

**Matrix x Matrix**

The naive approach to the matrix x matrix multiplication would be to devide matrix to the sub-metrixes. Broadcast the sub-matrix to the corresponding line/row and then compute the final submatrix.

The processors are connected in hypercube

Naive matrix multiplication|
|:-:|
![Naive matrix multiplication](img/PAG_matrix_mult.png)

The communication overhead is: 
$$2(t_s \log\sqrt{p} + t_w{n^2\over p}(\sqrt p -1))$$
Two broadcasts (over line and over column) of submatrix of size 
${n\over \sqrt p}\times {n\over\sqrt p}$

From that asymptotic paralel time is **APPROXIMATELY**:

$$T_p = {n^3\over p} + t_S \log p + 2t_w{n^2\over \sqrt p}$$

This is cost-optimal $\Theta(n^3)$ ;  $E=O(1)$

Isoeffitiency: after computing $T_o = p t_s \log p + 2t_wn^2\sqrt p$. We can split the isoeffitiency function into two

$$W_1 = Kpt_s\log p \to \Theta(p \log p)$$
$$W_2 = K2t_wW^{2\over 3}\sqrt p = \Theta(p^{3\over 2})$$

The isoeffitioncy is $\Theta(p^{3\over 2})$

Also it is not memory optimal

**Cannon algorithm**

The main principle of canons algorithm is that using cleaver swaps we will achieve the memory optimal algorithm, using the same parameters as the naive one.

Cannon algorithm|
|:-:|
![Cannon algoritm](img/PAG_cannon.png)

**DNS algorithm**

The base of the DNS algorithm is splitting the matrix into cube, where each plane represents single file.

DNS first part | DNS second part |
|:-:|:-:|
![DNS first part](img/PAG_DNS_matrix_multiplication.png)|![DNS second part](img/PAG_DNS_matrix_2.png)

**p=n^3**

In the pictures the solution with $p=n^3$ is displayed. 
Move each line and perform broadcast - $\log n$ time. 
Each processor computes add
computes single multiply and then accumulation is executed. The computation is constant, accumulation is again $\log n$ 

The total runtime is $T_p \in \Theta(\log n)$

It is not cost-optimal since $E = {S\over p} = {T_s\over pT_p} = {1\over log n}$

It can be cost-optimal using fewer processors:

**p<n^3**

Let $p=q^3$

1) One-to_one communication: $t_s+t_w({n\over q})^2$
2) Two (for matrix A and B) One-to-All broadcasts: $2\log q (t_s + t_w({n\over q})^2)$
3) Reduction: $\log q (t_s+t_w({n\over q})^2)$
4) Multipltication takes time $({n\over q})^3$

That will give us:

$$T_p = {n^3\over p} + (1+3\log \sqrt[3]p)(t_s+t_w({n\over \sqrt[3]p})^2)$$

The approximation (ignore sqrt in log and ignore constants):

$$T_p = {n^3\over p} + t_s\log p + t_w{n^2\over p^{2\over3}}\log p$$

The isoeffitiency:

$$T_o = pT_p - W = t_sp\log p + t_wp^{1\over3}n^2\log p$$

From the $t_w$ the isoeffitiency is $\Theta(p\log^3 p)$ 

 


### 8.4 Outline the principle of sorting networks and describe parallel bitonic sort, including its scalability. Explain parallel enumeration sort algorithm on PRAM model, including its scalability.

**Sorting networks**

The main building block of the sorting networks is comparator. The comparator has two inputs and two outputs. The increasing comparator takes the inputs *x* and *y* and outputs *x' = min{x, y}* and  y' = max{x,y}. The decreasiong has the outputs swapped. The increasing comparator is marked by $\oplus$ and decreasing by $\ominus$

**Bitonic sort**

The bitonic sort uses sorting network to sort the bitonic sequence, depicted in the *Bitonic Sort* picture. The speed of the algorithm is proportional to the depth of the sorting network. The bitonic merging network contains $\log n$ columns, each column containing $n\over 2$ comparators.

The depth of the network is 

$$d(n) = d({n\over2}) + \log n \to d(n) \in \Theta(\log^2n)$$

The serial implementation of the network would have the complexity of $\Theta(n \log^2 n)$

The bitonic merge network can be mapped to the hypercube, using the ID of the hypercube nodes as the Wire specified in Bitonic Sort network.

We need to creat a bitonic sequence out of the unsorted sequence. A sequence of length 2 is a bitonic sequence. The bitonic sequence of length 4 can be built by sorting the first two elements using $\oplus BM[2]$ and the next two by using $\ominus BM[2]$. This process can be repeated to create larger bitonic sequences.
The process is displayed in the *Creating bitonic sequence* picture.

Since each step takes $\Theta(1)$ time, the parallel time is $T_p = \Theta(\log^2 n)$

The algorithm is cost-optimal when compared to the serial implementation, but it is not cost-optimal, when compared with the best sorting serial algorithm.


Bitonic Sort| Creating Bitonic Sequence|
|:-:|:-:|
![Bitonic Sort](img/PAG_bitonic_sort.png)| ![Create bitonic sequence](img/PAG_create_bitonic.png)

**Enumeration sort**

The enumaration sort compares each element with every other element counting how many elements are smaller. Then places that element to the index that corresponds to the counted value. 

The parallel implementation takes $n^2$ processors in 2D grid. Each column represents one digit. Each processor executes one compare and then adds +1 if I am bigger or +0 to the common counter. Then the lemenet is placed to the index coresponding to the common counter.

This takes $\Theta(1)$ time.

### 8.5 Explain all steps of a parallel algorithm for finding connected components in a graph given by the adjacency matrix. Using an example, illustrate a parallel algorithm for finding a maximal independent set in a sparse graph.

**Finding of connected components in graph**

1) Graph is represented by the adjacency matrix. Devide that matrix into the processors, and each processor will compute the connected components for its nodes.
2) We need to combine created forrests of each processor. Merging forrest A and B uses unionfind approach. For each edge of A call *Find* to check if the vertices are in the same tree of B.
   1) If not they are united
   2) If yes, no unification is needed.
   
Merging of A and B requires at most 2(n-1) find opperations and (n-1) union operations $\to \Theta(n)$ and there are $\log p$ mergings. From that the asymptotic simplification is:

$$T_p = \overbrace{\Theta({n^2\over p})}^{\text{local compute}} + \overbrace{\Theta({n\log p})}^{forest merging}$$

From that Isoeffitioncy: $W=p^2\log^2p \to \Theta(p^2\log^2p)$ max number of processors is $p \in O({n\over \log n})$

**Maximal independent set**

Luby algorithm. Each node creates random number and shares that number to neighbours. If I have minimal number out of my neighbours, I am in the maximal set, remove neighbours, and iterate again. On average the algorithm converges after $O(\log |V|)$ steps. But it does not have to. Luby algorithm is very dependant on the numbering.

Graph for Luby|
|:-:|
![Graph for Luby](img/PAG_GRAPH_for_luby.png)

Example: in the above picture if we would use the numbers in the vertexes as a random value for the Luby algorithm, the maximum independant set would be {1,2,5,6}. That is not the biggest maximum independet set, that would be {1, 2, 7, 8, 11}. If we would colour the graph using Luby-Jones algorithm (same idea, just iteratively applied) the colouring would be following

| | RED | BLUE | GREEN | PURPLE 
|:-:|:-:|:-:|:-:|:-:|
Nodes|1, 2, 5, 6| 4, 5| 7, 11, 8| 9, 10

Which is not the optimal colouring, since the graph can be coloured using only three colours.

## 9. ESW - Effective algorithms and optimization methods. Data structures, synchronization and multithreaded programs.

### 9.1 Java Virtual Machine, memory layout, frame, stack-oriented machine processing, ordinary object pointer, compressed ordinary object pointer. JVM bytecode, Just-in-time compiler, tired compilation, on-stack replacement, disassembler, decompiler. Global and local safe point, time to safe point. Automatic memory Management, generational hypothesis, garbage collectors. CPU and memory profiling, sampling and tracing approach, warm-up phase.

### 9.2 Data races, CPU pipelining and superscalar architecture, memory barrier, volatile variable.  Synchronization - thin, fat and biased locking, reentrant locks. Atomic operations based on compare-and-set instructions, atomic field updaters. Non-blocking algorithms, wait free algorithms, non-blocking stack (LIFO).

### 9.3 Static and dynamic memory analysis, shallow and retained size, memory leak. Data Structures, Java primitives and objects, auto-boxing and unboxing, memory efficiency of complex data structures. Collection for performance, type specific collections, open addressing hashing, collision resolution schemes. Bloom filters, complexity, false positives, bloom filter extensions. Reference types - weak, soft, phantom.

### 9.4 JVM object allocation, thread-local allocation buffers, object escape analysis, data locality, non-uniform memory allocation.

### 9.5 Networking, OSI model, C10K problem. Blocking and non-blocking input/output, threading server, event-driven server. Event-based input/output approaches. Native buffers in JVM, channels and selectors.

### 9.6 Synchronization in multi-threaded programs (atomic operations, mutex, semaphore, rw-lock, spinlock, RCU). When to use which mechanism? Performance bottlenecks of the mentioned mechanisms. Synchronization in “read-mostly workloads”, advantages and disadvantages of different synchronization mechanisms.

### 9.7 Cache-efficient data structures and algorithms (e.g., matrix multiplication). Principles of cache memories, different kinds of cache misses. Self-evicting code, false sharing – what is it and how deal with it?

### 9.8 Profiling and optimizations of programs in compiled languages (e.g., C/C++). Hardware performance counters, profile-guided optimization. Basics of C/C++ compilers, AST, intermediate representation, high-level and low-level optimization passes.
